"""Launch the inference server."""

import os
import sys

from sglang.srt.entrypoints.http_server import launch_server
from sglang.srt.server_args import prepare_server_args
from sglang.srt.utils import kill_process_tree

# 服务器启动的入口，运行会启动一个HTTP服务器
# 该服务器会监听指定的端口，接收请求并返回响应
if __name__ == "__main__":
    # 解析命令行参数
    server_args = prepare_server_args(sys.argv[1:])
    # 解析完毕后是一个ServerArgs的数据类，结构如下：
    """
    ServerArgs(
        model_path='qwen/qwen2.5-0.5b-instruct',
        tokenizer_path='qwen/qwen2.5-0.5b-instruct',
        tokenizer_mode='auto',
        skip_tokenizer_init=False,
        enable_tokenizer_batch_encode=False,
        load_format='auto',
        trust_remote_code=False,
        dtype='auto',
        kv_cache_dtype='auto',
        quantization=None,
        quantization_param_path=None,
        context_length=None,
        device='cuda',
        served_model_name='qwen/qwen2.5-0.5b-instruct',
        chat_template=None,
        completion_template=None,
        is_embedding=False,
        revision=None,
        host='0.0.0.0',
        port=39215,
        mem_fraction_static=0.88,
        max_running_requests=None,
        max_total_tokens=None,
        chunked_prefill_size=2048,
        max_prefill_tokens=16384,
        schedule_policy='fcfs',
        schedule_conservativeness=1.0,
        cpu_offload_gb=0,
        page_size=1,
        tp_size=1,
        stream_interval=1,
        stream_output=False,
        random_seed=515058922,
        constrained_json_whitespace_pattern=None,
        watchdog_timeout=300,
        dist_timeout=None,
        download_dir=None,
        base_gpu_id=0,
        gpu_id_step=1,
        log_level='info',
        log_level_http=None,
        log_requests=False,
        log_requests_level=0,
        show_time_cost=False,
        enable_metrics=False,
        decode_log_interval=40,
        api_key=None,
        file_storage_path='sglang_storage',
        enable_cache_report=False,
        reasoning_parser=None,
        dp_size=1,
        load_balance_method='round_robin',
        ep_size=1,
        dist_init_addr=None,
        nnodes=1,
        node_rank=0,
        json_model_override_args='{}',
        lora_paths=None,
        max_loras_per_batch=8,
        lora_backend='triton',
        attention_backend=None,
        sampling_backend='flashinfer',
        grammar_backend='xgrammar',
        speculative_algorithm=None,
        speculative_draft_model_path=None,
        speculative_num_steps=None,
        speculative_eagle_topk=None,
        speculative_num_draft_tokens=None,
        speculative_accept_threshold_single=1.0,
        speculative_accept_threshold_acc=1.0,
        speculative_token_map=None,
        enable_double_sparsity=False,
        ds_channel_config_path=None,
        ds_heavy_channel_num=32,
        ds_heavy_token_num=256,
        ds_heavy_channel_type='qk',
        ds_sparse_decode_threshold=4096,
        disable_radix_cache=False,
        disable_cuda_graph=False,
        disable_cuda_graph_padding=False,
        enable_nccl_nvls=False,
        disable_outlines_disk_cache=False,
        disable_custom_all_reduce=False,
        enable_multimodal=None,
        disable_overlap_schedule=False,
        enable_mixed_chunk=False,
        enable_dp_attention=False,
        enable_ep_moe=False,
        enable_deepep_moe=False,
        deepep_mode='auto',
        enable_torch_compile=False,
        torch_compile_max_bs=32,
        cuda_graph_max_bs=8,
        cuda_graph_bs=None,
        torchao_config='',
        enable_nan_detection=False,
        enable_p2p_check=False,
        triton_attention_reduce_in_fp32=False,
        triton_attention_num_kv_splits=8,
        num_continuous_decode_steps=1,
        delete_ckpt_after_loading=False,
        enable_memory_saver=False,
        allow_auto_truncate=False,
        enable_custom_logit_processor=False,
        tool_call_parser=None,
        enable_hierarchical_cache=False,
        hicache_ratio=2.0,
        hicache_size=0,
        hicache_write_policy='write_through_selective',
        flashinfer_mla_disable_ragged=False,
        warmups=None,
        moe_dense_tp_size=None,
        n_share_experts_fusion=0,
        disable_chunked_prefix_cache=False,
        disable_fast_image_processor=False,
        debug_tensor_dump_output_folder=None,
        debug_tensor_dump_input_file=None,
        debug_tensor_dump_inject=False,
        disaggregation_mode='null',
        disaggregation_bootstrap_port=8998,
        disaggregation_transfer_backend='mooncake',
        disaggregation_ib_device=None
    )
    """

    try:
        # 启动HTTP服务器
        launch_server(server_args)
    finally:
        kill_process_tree(os.getpid(), include_parent=False)
